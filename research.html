<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="description" content="University of Verona - Department of Computer Science - VIPS Lab">
    <meta name="keywords"
          content="VIPS, reseach,computer vision, image processing, pattern recognition, machine learning">
    <meta name="author" content="Pietro Lovato">

    <link rel="shortcut icon" type="image/png" href="../../images/favicon.png"/>
    <title>Research</title>

    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <!-- Latest compiled JavaScript -->
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

    <!-- PERSONALIZED VIPS CSS AND SCRIPTS -->
    <link rel="stylesheet" type="text/css" href="./css/style.css">
    <script type="text/javascript" src="../../js/topnav.js"></script>
    <style type="text/css">
        p {
            font-size: 16px;
            margin: auto;
        }
    </style>

    <!-- GOOGLE ANALYTICS -->

</head>

<body bgcolor="#f0f8ff">

<nav>
    <div class="container">

    </div>
</nav>
<div class="title">
    <div class="title-top">
        <div class="title-left">
            <div class="title-right">
                <div class="title-bottom">
                    <div class="title-top-left">
                        <div class="title-bottom-left">
                            <div class="title-top-right">
                                <div class="title-bottom-right">
                                    <h1><a href="./index.html">MY <span>PERSONAL</span> WEBPAGE</a></h1>
                                    <h4><p><a href="./index.html"> <span>Sami Abduljalil Abdulhak</span></a></p></h4>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>


<h3></h3>

<div class="container mainmatter">
<div class="text-left">
    <div class="row">

        <div class="col-sm-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="index.html" class="col-sm-2" target="_self">Home</a></h4>
                    </div>
                </div>
            </div>
        </div>
        <div class="col-sm-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="research.html" class="col-sm-2" target="_self" class="active">Research</a></h4>
                    </div>
                </div>
            </div>
        </div>

        <div class="col-md-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="./publications.html" class="col-sm-2" target="_self">Publication</a></h4>
                    </div>
                </div>
            </div>
        </div>
        <div class="col-sm-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="./links.html" class="col-sm-2" target="_self">Applications</a></h4>
                    </div>
                </div>
            </div>
        </div>

        <div class="col-sm-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="http://picture.imcad.it/" class="col-sm-2" target="_blank">Photos</a></h4>

                    </div>
                </div>
            </div>
        </div>
        <div class="col-md-2">
            <div class="panel panel-default">
                <div class="panel-body">
                    <div class="people-descr">
                        <h4><a href="mailto:samialkindi0708@gmailcom" class="col-sm-2" target="_blank">Contact</a></h4>
                    </div>
                </div>
            </div>
        </div>


    </div>
</div>


<!-- MAIN BODY -->
<div class="container mainmatter" style="background-color: white">

<h2> Training Image Set Creation for Object Recognition</h2><br/>

<blockquote>
    <p> Object recognition is a process of recognizing a particular object in still images or videos, relying on
        matching & learning patterns extracted from a training images using
        appearance-based or feature-based computer vision techniques</p>
    <footer><cite title="Source Title"></cite></footer>
</blockquote>
<br>

<div class="text-left">
    <div class="row">

        <div class="col-sm-8">
            <p><em> Image Retrieval (IR)</em> is the process of finding and returning images from a huge image database
                based on the metadata such as keyword attached by human to describe their contents or
                based on the visual properties inferred from images such as color, shape, texture, etc. IR can be viewed
                as a more general problem than object recognition. For instance
                if you have an image retrieval system, and the query is an image
                of a dog, then the application would need to recognize that this is an image of a dog (object
                recognition), and then find other images of dogs in its database. However, there are
                several image retrieval platforms like <a target="blank"
                                                          href="https://www.google.it/imghp?hl=en&tab=wi&ei=k5CKVbXJKoWPygPog4wI&ved=0CBIQqi4oAQ">Google</a>
                enabling users to search by plain text or by visual contents and presenting similar images to
                the query. Yet, such platforms still return large portion of unrelated images to the query particularly
                when the supplied query is a complex "mouse".
                In other words, these platforms generally present the highest ranked images correctly relevant to the
                user intent, while the lower ranked images are somehow scattered from what the user desires to see.
                This problem occurs because image search engines like <a target="blank"
                                                                         href="https://www.google.it/imghp?hl=en&tab=wi&ei=k5CKVbXJKoWPygPog4wI&ved=0CBIQqi4oAQ">Google</a>
                unable to discriminate between semantics due to
                the polysemy or the ambiguity of these semantics. For example the concept "mouse" is an animal and also
                a computer device. Such problem is still greatly unsolved across different platforms like
                <a target="_blank" href="http://flickr.com">Flickr</a>, <a target="_blank"
                                                                           href="https://www.bing.com/images/search?q=mouse">Bing</a>,
                etc. On the other hand, content-based image retrieval applications rely heavily on the
                feature extracted from visual content using computer vision techniques for retrieval; avoiding the use
                of textual description. These features are inadequate to recognize and retrieve strongly coherent and
                related images to a user-supplied query image or user-specified image features.


            </p>

        </div>

        <div class="col-sm-4">
            <img id="ex_fform" align="right" style="border=0;" width="110%" height="260"
                 alt="Example of contaminated Retrieval Results" src="./images/noise.png">
        </div>
        <div class="col-sm-12">
            <p> Enabling machines to obtain images from a large image database autonomously for object recognition task
                while avoiding contaminated images as much as possible and able to handle the ambiguity and polysemy
                problems
                a strong technique is essentially needed.To gain a deep understanding of <em>
                    of the importance of the quality of the training image sets</em> for object recognition, we highly
                suggest to read the paper <a target="_blank"
                                             href="http://web.mit.edu/vondrick/largetrain.pdf">"Do We Need More Training
                    Data or Better
                    Models for Object Detection?"</a>. A detailed description of several algorithms is
                presented in the paper.</p>
        </div>
    </div>
</div>
<br><br><br>

<br><br>

<div class="text-left">

<h3 style="margin: 10px 0 5px;font-size:28px">Methods &nbsp; &nbsp;&nbsp;
    <button type="button" class="btn btn-info btn-sm" onclick="$('#target').toggle();">Show/Hide Content</button>
</h3>
<hr style="margin-top:0;margin-bottom:30px;">

<div id="target" style="display: none">
<div class="row">

    <div class="col-sm-9">
        <p>To build a training image set from Internet in an automatic way is greatly a challenging task involving
            semantic disambiguation and word polysemy. In general, when we build a good training image set for
            a specific object class for object recognition task, we normally query the Internet using a single
            keyword denoting the object class and downloading a set of predefined, possibly of relevant, images.
            However, the flaw escorts this way of generating a training image set is that using a single keyword, in
            general, presents a large portion of strange images, So, it is difficult to obtain visually
            correlated images due to the aforementioned problems (i.e., ambiguity and polysemy) accompanying some
            semantics. To overcome the semantic ambiguity when creating a good training set from Internet, we need
            to
            label the keyword, before crawling images, with the most appropriate lexical relations such as "ISA"
            relation. These relations should be able to disambiguate semantics such as "jet" is <em>kind of</em>
            "aeroplane" object or "wheels" are <em>part of</em> "car" object. These relations are known as <em>hyperonymy </em>
            or <em>hyponymy</em> and a few more publicly available in
            <a target="_blank" href="https://wordnet.princeton.edu/">WordNet</a>. </p>
    </div>


    <div class="col-sm-3">
        <img id="gcff_schema" align="right" style="border=0;" width="100%" height="210"
             alt="Example of refined result" src="./images/clean_search.png">
    </div>
    <div class="col-sm-12">
        <p> WordNet is a widely known large lexical
            database of English where terms are grouped into sets of cognitive synonyms(synsets) each expressing a
            distinct concepts. It is viewed as a network whose nodes are sysnsets and arcs are semantic relations.
            On other hand, polysemous keywords are tricky and can result in scattered image collection when used as
            a searching query. Thus to avoid the multiple meanings of any polyesmous keyword it is very smart and
            necessary to properly select one or more
            surrounding words to identify the intended visibility of the keyword. For example, adding the word <em>
                "animal"</em> to the keyword "mouse" is expected to result in less contaminated images (see the
            example). Furthermore,
            the selection of the surrounding term should also be done autonomously. </p>

        <p>To generate a good training image set for object recognition tasks while essentially preserving the
            generalization capabilities, we propose over the past three years several unique approaches. For more
            details on the approaches,
            please read the papers below.</p>
    </div>

</div>

<!-- approaches -->

<br><br>
<h4 style="font-size:22px;margin-bottom:15px;"><b>Lexical and Ontological Search(LOS)</b></h4>

<div class="row">
    <div class="col-sm-12">
        <p>Under this approach, we uniquely propose to exploit the publicly available lexical database like WordNet
            to select appropriate terms to be attached to the main keyword (i.e., object class name) for crawling
            images from Internet.
            The general idea is to exploit semantic technologies for refining the search results. So, our proposal
            comprises of three main components:</p>
        <dl>
            <dd><p><b>&nbsp;-Lexical Search:</b> We use WordNet as a lexical database to automatically generate a
                set of related terms to the main keyword (<var>C<sub>L</sub></var>). WordNet can automatically
                generate three distinct sets of related terms, connected to the original one by means of three basic
                conceptual relations: hyperonymy, hyponymy and meronymy (For more details please refer
                to <a target="_blank" href="https://wordnet.princeton.edu/">WordNet</a>) . </p></dd>
            <dd><p><b>&nbsp;-Ontological Search:</b> We use ontological search to prune out visually meaningless
                terms from the set generated by WordNet. For example, within
                meronyms we can have internal or external parts: thinking about a generic means of transportation,
                while wheels are external parts, usually visible, that can help a recognizer to recognize the class,
                engines are internal parts that do not contribute with any essential information about the outlook
                of the object. </p></dd>
            <dd><p><b>&nbsp;-Image Search:</b> Given the name of the object class <em>C</em> and the set of
                connected terms we can perform an internet image search
                with any web search engine which allows image specification. The core of our algorithm is to use as
                keywords for the search
                engine the set of <em>i</em> composed keywords <var>K<sub>i</sub></var> defined as the pairs
                <var>K<sub>i</sub></var> = <em>C</em> + <var>C<sub>L</sub></var>,<var>O<sub>i</sub></var>, where the
                symbol '+' is the string concatenation operator.</p></dd>
        </dl>

    </div>
</div>

<br>

<div class="row">
    <div class="col-sm-7">
        <img id="hvff_schema" align="left" style="border=0;margin-top:20px" width="100%" height="250"
             alt="LOS Schema" src="./images/los-approach.png">

        <p style="font-size:80%;line-height:20px">Methodological representation of the proposed algorithm. Given the
            class name <em>C</em>, the lexical
            search returns a set of related terms <var>C<sub>L</sub></var>, ontological search selects only the
            terms which are visually meaningful <var>C<sub>L</sub></var>, O, while an internet image search
            engine is used to crawl for images <var>I<sub>C</sub></var> by using these pair of terms as a keyword
        </p>
    </div>

    <div class="col-sm-5">
        <p>As a first step, we explore the effectiveness of the semantic technologies for refining the search result
            when quering Internet
            for images to be used as a training set for object recognition task. We build a training images of 10
            classes randomly selected from Caltech101 public dataset; using our proposed framework.
            We build 1000 visual words based on <a target="_blank"
                                                   href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform/">SIFT</a>
            descriptors for both training and testing sets,
            represent each image in terms of histogram of visual words (<a target="_blank"
                                                                           href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">
                Bag of Words Model</a>), and consider single-class
            support vector machine (SVM) to distinguish one class of data from a set of multiple classes. This work
            has been presented at VIGTA 2013
            <a href="#bmvc11">[1]</a>;
            </br> A followed work has also been presented at ICIAP 2013 as an improvement of the proposed approach
            by using more object classes (44 object class of Caltech). The idea is to observe if our approach still
            leverages the
            recognition performance when different challenges escorting some object classes. Our experiments greatly
            shows constant improvement of recognition accuracy compared to the traditional methods.
            <a href="#wiamis13">[2]</a></p>
    </div>
</div>

<br>
<h4 style="font-size:22px;margin-bottom:15px;"><b>Textual Tags Analysis (TTA)</b></h4>

<div class="row">
    <div class="col-sm-12">
        <p> Here, our goal is to take advantage of the textual tags given by humans to images describing their
            contents in social websites like <a target="_blank" href="http://flickr.com">Flickr</a>
            to automatically select the most representative of an object category for
            training a classifier, without looking at the nature of the objects therein. For example, there exists
            particular underlying
            connection/association between the textual information and visual properties modalities which utilizing them
            would help to identify
            some hidden patterns that can leverage the recognition accuracy. From this observation, we propose to
            exploit both semantic analysis and pure statistical approaches. These
            considerations lead us to focus on three main features:
        <dl>
            <dd><p><b>&nbsp;-Keyword Position:</b> To capture an image as related or unrelated on the basis of a
                keyword(i.e., object class name) position in a tag list.
                For instance, given a tag list comprises of a number of textual tags and corresponds to a particular
                image, the algorithm tries to search for
                the keyword through the list in the first <var>n<sup>th</sup></var> positions. The algorithm then
                labels the image as positive (reliable) if it is related to
                the class name or negative (outlier); </p></dd>
            <dd><p><b>&nbsp;-Semantic Distance:</b> We compute the semantic relatedness by means of semantic
                distance measures.
                For instance, our approach takes the object class (represented by a keyword) and each image's tag
                list,
                then computes the distance of the keyword to every single textual tag in a tag list, yielding a
                score for each. In the case where there exists no
                semantic distance between the keyword and a textual tag, our approach discards a tag from a list.
                Therefore our approach labels an image as positive (reliable) if
                its accumulated tags score in it tags list is equal or above a threshold; otherwise as negative
                (outlier)</p></dd>
            <dd><p><b>&nbsp;-Tag Frequency:</b> We count the frequency occurrence of each single tag of all tag list
                of the object class.
                Our approach uses the frequency values of each tag in a tag list to determine if a given image is
                related to the object class or otherwise. Precisely, it combines the frequency
                value of each <var>tag<sub>(i,j)</sub></var> to produce a score. Then, it labels an image <em>i</em>
                as positive (reliable) if its score is equal or above a
                threshold; otherwise negative (outlier).</p></dd>
        </dl>
        </p>
        <div class="row">
            <div class="col-sm-7">
                <p>To begin under this caption, first we pool images <var>I<sub>n</sub></var> for
                    a set of 21 object classes taken from the standard
                    <a target="_blank"
                       href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech101</a>. We defined
                    <em>n</em> to 400 as well as their corresponding tag lists <var>tag<sub>i</sub></var>
                    descriptors for both training and testing sets. Furthermore, we build a graphic user interface
                    (GUI) that allows user to manually label images as positive or as negative to the object class.
                    To perform a manual classification, certain guidelines are defined and adopted which of the
                    following guidelines are satisfied, the image is labeled as negative; otherwise as positive:
                    <br>

                <p>&nbsp;-an image is completely unrelated with the object specified by the category it
                    belongs;</p>

                <p>&nbsp;-an image contains irrelevant parts of the object, that is, parts that alone are not
                    sufficient to make the category object identifiable;</p>

                <p>&nbsp;-an image contains only internal parts of the category object (like a cockpit of an
                    aeroplane or an engine of a car);</p>

                <p>&nbsp;-an image is drawings and caricatures of the category object.</p>  <br/>

                <p> We evaluate the performance of the proposed approach
                    against manually labels images. The result of this work has been presented in ECCV workshop <a
                            href="#icip13">[3]</a>. </p>

                </p>
            </div>
            <div class="col-sm-5">
                <img id="hvff_schema" align="left" style="border=0;margin-top:20px" width="100%" height="250"
                     alt="LOS Schema" src="./images/TTA.png">

                <p style="font-size:80%;line-height:20px">Schematic representation of the proposed framework. Our
                    approach
                    starts by collecting images from Flickr, labels images manually, performs semantic analysis,
                    generates a postive and a negative images sets,
                    and finally evaluates the propose generated sets against the manually labeled ones.
                </p>
            </div>
            <div class="col-sm-12">
                <p>A followed work is also conducted to investigate the use of textual tags coming with images for
                    query expansion.
                    We proposed several new features to automatically select candidate and useful tags to expand the
                    query when generating a training image set for image classification.
                    The results show a slight improvement in the recognition accuracy compared with recent state of
                    the art method; working in the same paradigm. A live demo of
                    our approach has been developed and publicly available for use <a target="_blank"
                                                                                      href="http://klimble.com/sami/IMCAD/">IMCAD</a>.
                    The results of this work have been
                    accepted in ICIAP 2015<a href="#gcff">[4]</a></p>
            </div>
        </div>
    </div>

    <br><br/>

    <div class="row">
        <div class="col-sm-12">
            <h4 style="font-size:22px;margin-bottom:15px;"><b>Big-data for Image Training Sets Creation(BITS)</b></h4>


            <p>Recently the term BIG DATA is being widely spread in many tech companies and academia. This concept is
                used to describe the exponential growth and availability of
                data in many forms like structured(e.g., database) or unstructured(e.g., html, documents, etc) forms
                that is difficult to process using the conventional databases or some techniques in order to uncover
                hidden patterns that can assist in leveraging/improving/predicting/avoiding (you name it) the
                performance of a specific task scenario. For big companies, <i> BIG DATA</i> is potentially important
                which can help
                them to make up faster, intelligent decisions (I can also it forecasting the market trend futurely)</p>
        </div>
    </div>


    <div class="row">
        <div class="col-sm-12">
            <h4 style="font-size:22px;margin-bottom:15px;"><b>Semantic Similarity for Training Sample Selection</b></h4>


            <p>Recently the term BIG DATA is being widely spread in many tech companies and academia. This concept is
                used to describe the exponential growth and availability of
                data in many forms like structured(e.g., database) or unstructured(e.g., html, documents, etc) forms
                that is difficult to process using the conventional databases or some techniques in order to uncover
                hidden patterns that can assist in leveraging/improving/predicting/avoiding (you name it) the
                performance of a specific task scenario. For big companies, <i> BIG DATA</i> is potentially important
                which can help
                them to make up faster, intelligent decisions (I can also it forecasting the market trend futurely)</p>
        </div>
    </div>

</div>
</div>
<br><br>

<div class="text-left">
    <h3 style="margin: 10px 0 5px;font-size:28px"> Code &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
        <button type="button" class="btn btn-info btn-sm" onclick="$('#target1').toggle();">Show/Hide Content</button>
    </h3>
    <hr style="margin-top:0;margin-bottom:30px;">
    <div id="target1" style="display: none">
        <div class="row">
            <div class="col-sm-12">
                <p>The code for both <b>LOS</b> and <b>TTA</b> is publicly available under the GPL license. Everyone
                    can use this code for research purposes. If you publish results of the research, please cite the
                    related papers as reported in the README.txt file inside the zip folder.</p><br>
                <ul>
                    <li><b>Lexical and Ontological Search (LOS): <a target="blank" href="">TBU</a></b></br></br>
                    </li>
                    <li><b>Textual Tag Analysis(TTA): <a target="blank"
                                                         href="">TBU</a></b></br></br>
                    </li>
                </ul>
            </div>
        </div>
    </div>
</div>


<br>

<div class="text-left">

    <h3 style="margin: 10px 0 5px;font-size:28px">Datasets &nbsp; &nbsp;
        <button type="button" class="btn btn-info btn-sm" onclick="$('#target2').toggle();">Show/Hide Content</button>
    </h3>
    <hr style="margin-top:0;margin-bottom:30px;">
    <div id="target2" style="display: none">
        <div class="row">
            <div class="col-sm-12">
                <p>Several detection and recognition datasets are publicly available which we used to evaluate our
                    proposed
                    methodologies. These datasets are tabulated and described bellow
                </p><br>

                <center>
                    <div class="table-responsive">
                        <table class="table table-bordered">
                            <thead>
                            <tr>
                                <th>Dataset</th>

                                <th>Recognition Accuracy</th>
                            </tr>
                            </thead>

                            <tr>
                                <td style="text-align:left"><i>CALTECH101 dataset</i></td>

                                <td>very high</td>
                            </tr>
                            <tr>
                                <td style="text-align:left"><i>CALTECH256</i></td>

                                <td>high</td>
                            </tr>
                            <tr>
                                <td style="text-align:left"><i>PASCALVOC2012</i></td>

                                <td>low</td>
                            </tr>
                        </table>
                    </div>
                </center>
                <br>

                <ul>

                    <li><b>CALTECH101 DATASET</b> (<a target="blank"
                                                      href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">link</a>):
                        Pictures of objects belonging to 101 categories. About 40 to 800 images per category.
                        Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco
                        Andreetto, and Marc 'Aurelio Ranzato. The size of each image is roughly 300 x 200
                        pixels.</br></br></li>
                    <li><b>CALTECH256 dataset</b> (<a target="blank"
                                                      href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">link</a>):
                        Caltech-256 is collected from two popular online databases. It represent a diverse set of view
                        conditions like lighting, poses, back-grounds, images sizes, and also camera systematics. Its
                        categories
                        are hand-crafted by the authors to represent a wide variety of natural and artificial objects in
                        various settings.</br></br></li>
                    <li><b>PASCALVOC2012 dataset</b> (<a target="blank"
                                                         href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.htmll">link</a>):
                        The Pascal Visual Object Classes dataset is a well known dataset of natural images for
                        classification and object detection</br></br></li>
                </ul>

            </div>
        </div>
    </div>
</div>
<br><br>

<h3 style="margin: 10px 0 5px;font-size:28px">References
    <button type="button" class="btn btn-info btn-sm" onclick="$('#target3').toggle();">Show/Hide Content</button>
</h3>
<hr style="margin-top:0;margin-bottom:30px;">
<div id="target3" style="display: none">
    <div class="row">
        <div class="col-sm-12">

            <ol align="left">
                <li><a id="bmvc11"><b>Tell me more: how semantic technologies can help refining internet image
                    search</b></a><br/>
                    F Setti, D Porello, R Ferrario, SA Abdulhak, M Cristani<br/>

                    Proceedings of the International Workshop on Video and Image Ground Truth in Computer Vision
                    Applications (VIGTA 2013), St. Petersburg, AA, Russian Federation, 2013<br/>
                    <a class="btn btn-primary" style="margin:5px 5px 10px 5px;" TARGET="_blank"
                       href="http://www.loa.istc.cnr.it/sites/default/files/vigta13_ontoclass.pdf">PDF</a>
                    <a class="btn btn-danger" style="margin:5px 5px 10px 5px;" data-toggle="collapse" href="#bib1"
                       aria-expanded="false" aria-controls="collapseExample">
                        Bibtex
                    </a>
                    <a class="btn btn-success" style="margin:5px 5px 10px 5px;"
                       href="http://dl.acm.org/citation.cfm?id=2501110" target="_blank">Web</a>
                    <a class="btn btn-warning" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_22" target="_blank">PPT</a>
                    <br/>

                    <div class="collapse" id="bib1">

          <pre>
<code>@inproceedings{Setti:2013:TMM:2501105.2501110,
    author = {Setti, Francesco and Porello, Daniele and Ferrario, Roberta and Abdulhak, Sami Abduljalil and Cristani,
    Marco},
    title = {"Tell Me More": How Semantic Technologies Can Help Refining Internet Image Search},
    booktitle = {Proceedings of the International Workshop on Video and Image Ground Truth in Computer Vision
    Applications},
    series = {VIGTA '13},
    year = {2013},
    isbn = {978-1-4503-2169-3},
    location = {St. Petersburg, Russia},
    pages = {3:1--3:6},
    articleno = {3},
    numpages = {6},
    url = {http://doi.acm.org/10.1145/2501105.2501110},
    doi = {10.1145/2501105.2501110},
    acmid = {2501110},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {internet image search, lexical resources, ontologies},
    }
    }</code>
          </pre>

                    </div>
                    <br/>
                </li>

                <li><a id="wiamis13"><b>Ontology-Assisted Object Detection: Towards the Automatic Learning with
                    Internet</b></a><br/>
                    Francesco Setti, Dong-Seon Cheng, Sami Abduljalil Abdulhak, Roberta Ferrario, Marco
                    Cristani<br/>
                    17th International Conference on Image Analysis and Processing (ICIAP 2013), Naples, Italy,
                    September, 2013<br/>
                    <a class="btn btn-primary" style="margin:5px 5px 10px 5px;"
                       href="http://www.loa.istc.cnr.it/sites/default/files/iciap13_ontoclass.pdf">PDF</a>
                    <a class="btn btn-danger" style="margin:5px 5px 10px 5px;" data-toggle="collapse" href="#bib2"
                       aria-expanded="false" aria-controls="collapseExample">
                        Bibtex
                    </a>
                    <a class="btn btn-success" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-642-41184-7_20" target="_blank">Web</a>
                    <a class="btn btn-warning" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_22"
                       target="_blank">PPT</a><br/>

                    <div class="collapse" id="bib2">

          <pre>
<code>@InCollection{raey,
    Title = {Ontology-Assisted Object Detection: Towards the Automatic Learning with Internet},
    Author = {Setti, Francesco and Cheng, Dong-Seon and Abdulhak, SamiAbduljalil and Ferrario, Roberta and Cristani,
    Marco},
    Booktitle = {Image Analysis and Processing Ã¢â‚¬â€œ ICIAP 2013},
    Publisher = {Springer Berlin Heidelberg},
    Year = {2013},
    Editor = {Petrosino, Alfredo},
    Pages = {191-200},
    Series = {Lecture Notes in Computer Science},
    Volume = {8157},

    Doi = {10.1007/978-3-642-41184-7_20},
    ISBN = {978-3-642-41183-0},
    Keywords = {object detection; one-class SVM; machine learning; ontology; semantic search},
    Language = {English},
    Url = {http://dx.doi.org/10.1007/978-3-642-41184-7_20}
    }

</code>
          </pre>

                    </div>
                    <br/>
                </li>

                <li><a id="icip13"><b>Semantic-Analysis Object Recognition: Automatic Training Set Generation Using
                    Textual Tags</b></a><br/>
                    Sami Abduljalil Abdulhak, Walter Riviera, Nicola Zeni, Matteo Cristani, Roberta Ferrario, Marco
                    Cristani<br/>
                    Computer Vision - (ECCV 2014) Workshops, Zurich, Switzerland, 2014<br/>
                    <a class="btn btn-primary" href="http://2013.ieeeicip.org/proc/pdfs/0003547.pdf"
                       style="margin:5px 5px 10px 5px;">PDF</a>
                    <a class="btn btn-danger" style="margin:5px 5px 10px 5px;" data-toggle="collapse" href="#bib3"
                       aria-expanded="false" aria-controls="collapseExample">
                        Bibtex
                    </a>
                    <a class="btn btn-success" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_22" target="_blank">Web</a>
                    <a class="btn btn-warning" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_22"
                       target="_blank">PPT</a><br/>

                    <div class="collapse" id="bib3">

          <pre>
<code>@InCollection{raey,
    Title = {Semantic-Analysis Object Recognition: Automatic Training Set Generation Using Textual Tags},
    Author = {Abdulhak, SamiAbduljalil and Riviera, Walter and Zeni, Nicola and Cristani, Matteo and Ferrario, Roberta
    and Cristani, Marco},
    Booktitle = {Computer Vision - ECCV 2014 Workshops},
    Publisher = {Springer International Publishing},
    Year = {2015},
    Editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
    Pages = {309-322},
    Series = {Lecture Notes in Computer Science},
    Volume = {8926},

    Doi = {10.1007/978-3-319-16181-5_22},
    ISBN = {978-3-319-16180-8},
    Keywords = {Training set; Semantic; Ontology; Semantic similarity; Image retrieval; Textual tags; Flickr; Object
    recognition},
    Language = {English},
    Url = {http://dx.doi.org/10.1007/978-3-319-16181-5_22}
    }
</code>
          </pre>

                    </div>
                    <br/>
                </li>

                <li><a id="gcff"><b>Crowdsearching Training Sets for Image Classification</b></a></br>
                    Sami Abduljalil Abdulhak, Walter Riviera, Marco Cristani</br>
                    18th International Conference on Image Analysis and Processing (ICIAP 2015), Genova, Italy,
                    2015</br>
                    <a class="btn btn-primary" style="margin:5px 5px 10px 5px;"
                       href="http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0123783&representation=PDF">PDF</a>
                    <a class="btn btn-danger" style="margin:5px 5px 10px 5px;" data-toggle="collapse" href="#bib4"
                       aria-expanded="false" aria-controls="collapseExample">
                        Bibtex
                    </a>
                    <a class="btn btn-success" style="margin:5px 5px 10px 5px;"
                       href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0123783"
                       target="_blank">Web</a>
                    <a class="btn btn-warning" style="margin:5px 5px 10px 5px;"
                       href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_22"
                       target="_blank">PPT</a><br/>

                    <div class="collapse" id="bib4">

          <pre>
<code>@article{Sami, 2012
    title = {Crowdsearcing Training Sets for Image Classification},
    author = { Sami Abduljalil Abdulhak, Walter Riviera, Marco Cristani},
    journal = {},
    volume = {},
    number = {},
    year = {2015}
    }</code>
          </pre>

                    </div>
                    <br/>
                </li>
            </ol>
        </div>
    </div>

    <br><br>
    <hr style="margin-top:0;margin-bottom:30px;">

    <div class="row">
    </div>

    <br/><br/><br/><br/><br/>

</div>
</div>
<br/>
<!-- CLUSTERATION ---- -->
<div class="text-left">

<h2 class="text-center"> Clusterization </h2>
<hr style="margin-top:0;margin-bottom:30px;">
<div class="row">
    <p>The term cluster analysis (first used by Tryon, 1939) encompasses a number of different algorithms and
        methods for grouping objects of similar kind into respective categories.
        A general question facing researchers in many areas of inquiry is how to organize observed data into
        meaningful structures, that is, to develop taxonomies.
        In other words cluster analysis is an exploratory data analysis tool which aims at sorting different objects
        into groups in a way that the degree of association between
        two objects is maximal if they belong to the same group and minimal otherwise. Given the above, cluster
        analysis can be used to discover structures in data without providing an
        explanation/interpretation. In other words, cluster analysis simply discovers structures in data without
        explaining why they exist.
        We deal with clustering in almost every aspect of daily life. For example, a group of diners sharing the
        same table in a restaurant may be regarded as a cluster of people.
        In food stores items of similar nature, such as different types of meat or vegetables are displayed in the
        same or nearby locations. There is a countless number of examples
        in which clustering plays an important role. For instance, biologists have to organize the different species
        of animals before a meaningful description of the differences between
        animals is possible. According to the modern system employed in biology, man belongs to the primates, the
        mammals, the amniotes, the vertebrates, and the animals.
        Note how in this classification, the higher the level of aggregation the less similar are the members in the
        respective class. Man has more in common with all other primates
        (e.g., apes) than it does with the more "distant" members of the mammals (e.g., dogs), etc. For a review of
        the general categories of cluster analysis methods, see Joining (Tree Clustering),
        Two-way Joining (Block Clustering), and k-Means Clustering. In short, whatever the nature of your business
        is, sooner or later you will run into a clustering problem of one form or another.</p>
</div>
<br><br><br>

<br><br>
<br><br>

<div class="text-left">

    <h3 style="margin: 10px 0 5px;font-size:28px">Methodology &nbsp; &nbsp;&nbsp;
        <button type="button" class="btn btn-warning btn-sm" onclick="$('#target4').toggle();">Show/Hide Content
        </button>
    </h3>
    <hr style="margin-top:0;margin-bottom:30px;">
    <div id="target4" style="display: none">
        <h4 style="font-size:22px;margin-bottom:15px;"><b>Standard Approaches</b></h4>

        <div class="row">
            <div class="col-sm-12">
                <h2><a name="k"></a><i>k</i>-Means Clustering</h2>
                <ul>
                    <li><a href="">Example</a></li>
                    <li><a href="">Computations</a></li>
                    <li><a href="">Interpretation of results</a></li>
                </ul>
                <h3>General Logic</h3>

                <p>This method of clustering is very different from the oining (Tree Clustering)and Two-way Joining</a>.
                    Suppose that you already have hypotheses concerning the number of clusters in your cases or
                    variables. You may want to "tell" the computer to form exactly 3 clusters that are to be as distinct
                    as possible. This is the type of research question that can be addressed by the k- means clustering
                    <a href="/textbook/statistics-glossary/a.aspx?button=a#Algorithm">algorithm</a>. In general, the <i>k</i>-means
                    method will produce exactly <i>k</i> different clusters of greatest possible distinction. It should
                    be mentioned that the best number of clusters <i>k</i> leading to the greatest separation (distance)
                    is not known as <i>a priori</i> and must be computed from the data (see Finding the Right Number of
                    Clusters). </p>

                <h3>Example</h3>

                <p>In the physical fitness example (see Two-way Joining), the medical researchers may have a "hunch"
                    from clinical experience that their heart patients fall basically into three different categories
                    with regard to physical fitness. They might wonder whether this intuition can be quantified, that
                    is, whether a <i>k</i>-means cluster analysis of the physical fitness measures would indeed produce
                    the three clusters of patients as expected. If so, the means on the different measures of physical
                    fitness for each cluster would represent a quantitative way of expressing the researchers'
                    hypothesis or intuition (i.e., patients in cluster 1 are high on measure 1, low on measure 2, etc.).
                </p>

                <h3>Computations</h3>

                <p>Computationally, you may think of this method as analysis of variance (ANOVA) "in reverse." The
                    program will start with <i>k</i> random clusters, and then move objects between those clusters with
                    the goal to 1) minimize variability within clusters and 2) maximize variability between clusters. In
                    other words, the similarity rules will apply maximally to the members of one cluster and minimally
                    to members belonging to the rest of the clusters. This is analogous to "ANOVA in reverse" in the
                    sense that the significance test in ANOVA evaluates the between group variability against the
                    within-group variability when computing the significance test for the hypothesis that the means in
                    the groups are different from each other. In <i>k</i>-means clustering, the program tries to move
                    objects (e.g., cases) in and out of groups (clusters) to get the most significant ANOVA results.
                </p>

                <h3>Interpretation of Results</h3>

                <p>Usually, as the result of a <i>k</i>-means clustering analysis, we would examine the means for each
                    cluster on each dimension to assess how distinct our <i>k</i> clusters are. Ideally, we would obtain
                    very different means for most, if not all dimensions, used in the analysis. The magnitude of the <i>F</i>
                    values from the analysis of variance performed on each dimension is another indication of how well
                    the respective dimension discriminates between clusters.
                <table align="right">
                    <tbody>
                    <tr>
                        <td></td>
                    </tr>
                    </tbody>
                </table>
                <br/>
                <br clear="right"/>
                &#160;</p>
            </div>
        </div>

        <hr style="margin-top:0;margin-bottom:30px;">
        <div class="row">
            <br/>

            <div class="col-sm-4">
                <style type="text/css">
                    .tg {
                        border-collapse: collapse;
                        border-spacing: 0;
                    }

                    .tg td {
                        font-family: Arial, sans-serif;
                        font-size: 14px;
                        padding: 10px 5px;
                        border-style: solid;
                        border-width: 1px;
                        overflow: hidden;
                        word-break: normal;
                    }

                    .tg th {
                        font-family: Arial, sans-serif;
                        font-size: 14px;
                        font-weight: normal;
                        padding: 10px 5px;
                        border-style: solid;
                        border-width: 1px;
                        overflow: hidden;
                        word-break: normal;
                    }

                    .tg .tg-c9cr {
                        font-style: italic
                    }

                    .tg .tg-e3zv {
                        font-weight: bold
                    }
                </style>
                <table class="tg">
                    <tr>
                        <th class="tg-e3zv">classes</th>
                        <th class="tg-e3zv">SimpleFlickr</th>
                        <th class="tg-e3zv">thre=Median</th>
                        <th class="tg-e3zv">thre=Mean</th>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">aeroplane</td>
                        <td class="tg-e3zv">0.95</td>
                        <td class="tg-e3zv">0.95</td>
                        <td class="tg-031e">0.94</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">bus</td>
                        <td class="tg-031e">0.78</td>
                        <td class="tg-e3zv">0.82</td>
                        <td class="tg-031e">0.60</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr"> car</td>
                        <td class="tg-031e">0.62</td>
                        <td class="tg-031e">0.59</td>
                        <td class="tg-e3zv"> 0.66</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">cat</td>
                        <td class="tg-031e">0.83</td>
                        <td class="tg-031e">0.80</td>
                        <td class="tg-e3zv">0.84</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">motorbike</td>
                        <td class="tg-031e">0.71</td>
                        <td class="tg-031e">0.67</td>
                        <td class="tg-e3zv"> 0.72</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">person</td>
                        <td class="tg-031e">0.39</td>
                        <td class="tg-031e">0.33</td>
                        <td class="tg-e3zv"> 0.51</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">sofa</td>
                        <td class="tg-031e">0.21</td>
                        <td class="tg-031e">0.34</td>
                        <td class="tg-e3zv"> 0.37</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">train</td>
                        <td class="tg-031e">0.75</td>
                        <td class="tg-031e">0.65</td>
                        <td class="tg-e3zv">0.78</td>
                    </tr>
                    <tr>
                        <td class="tg-c9cr">tv</td>
                        <td class="tg-031e">0.60</td>
                        <td class="tg-031e">0.65</td>
                        <td class="tg-e3zv"> 0.67</td>
                    </tr>
                    <tr>
                        <td class="tg-e3zv">AP</td>
                        <td class="tg-031e">~</td>
                        <td class="tg-031e">~</td>
                        <td class="tg-031e">~</td>
                    </tr>
                </table>
            </div>

            <div class="col-sm-8">
                <p>The following table shows the classification performance on three different sets: flickr, median,
                    mean sets. First,
                    the simple flickr dataset is simply the retrieved result returned by Flickr using the object
                    class name as a query.In the second and the third sets,
                    we use all the samples generated by Flickr and perform clusterization applying the standard
                    k-means. The number of clusters is set to 10 cluster per class.
                    To select the clusters which are useful and similar, we apply similarity distance between
                    clusters, keeping the minimum distance.
                    We think that cluster that has a closer distance to other clusters is highly representative
                    while the cluster that is far by distance is considered abnormal.</p> <br/>
            </div>
        </div>
    </div>
</div>

<!-- Deep learning ---- -->
<div class="text-left">

    <h2 class="text-center"> Deep Learning </h2>
    <hr style="margin-top:0;margin-bottom:30px;">
    <div class="row">
        <p>Recently the concept of deep learning is being popularized and utilized in many areas . The media frequently
            reports on talent acquisitions in this field, such as those by Google and Facebook, and startups which
            claim to employ deep learning are met with enthusiasm. Gratuitous comparisons with the human brain are
            frequent. But is this just a trendy buzz word? What exactly is deep learning and how is it relevant to
            developments in machine intelligence?

            For many researchers, deep learning is simply a continuation of the multi-decade advancement in our
            ability to make use of large scale neural networks. Letâ€™s first take a quick tour of the problems that
            neural networks and related technologies are trying to solve, and later we will examine the deep
            learning architectures in greater detail.

            Machine learning generally breaks down into two application areas which are closely related:
            classification and regression.

            In the classification task, you are trying to do automatic recognition. You create a training data set
            for which you have known labels, for example, images of different types of vegetables. Here you have
            manually assigned the correct class label, such as yam, carrot, potato, etc, to each one. The images are
            going to be the input to the algorithm and the class labels are going to be the required output.

            During the training phase you show the images to the algorithm and automatically adjust the many
            parameters in order that the generated output best matches the known labels. Then, when this is done,
            you hope that the algorithm will be able to generalize its classification ability to yield correct
            labels for new images which were not in the training set. You can evaluate this capability on a
            different hand-labeled data set which is used for testing. This is the classic framework that is used
            for face, object, or voice recognition, or for flagging the presence of particular patterns, e.g. in
            face detection.

            Regression, on the other hand, deals with continuous outputs where the problem is to compute an
            important quantity from any particular input vector.

            For example I might supply a list of latitudes and longitudes and want the algorithm to predict the
            height of the terrain at that location. The algorithm would be trained by giving it a limited set of
            spot heights at various coordinates, and it would have to generalize in order to fill in the missing
            regions when queried about any location that was not in the training set.

            The simplest example of regression is fitting a line through some data points, but more complex
            applications include evaluating the 3D pose of a face, or computing human joint angles, as is done by
            the Microsoft Kinect. Regression learning can be used as part of a control system for robotics, e.g. in
            following a path through an environment, or in computing motion in a scene.

            The kinds of techniques used for regression and classification are very similar because both involve
            multidimensional generalization beyond training data and the difference is just whether the output is
            discrete or continuous.

            In order to actually make an algorithm that can learn, one tries to come up with a super general black
            box which has many parameters (think of control knobs) that can be twiddled in order to change the
            mapping from its input to its output. Then the age-old techniques of gradient descent are used to
            automatically tune all these parameters to get the best results possible on the training set. Much of
            the art of machine learning research lies in inventing the contents of the black box and in designing
            the optimization algorithm so that training converges to good solutions in a reasonable time.
        </p></div>
    <br><br><br>

    <br><br>
    <br><br>

    <div class="text-left">
        <h3 style="margin: 10px 0 5px;font-size:28px">State of The arts &nbsp; &nbsp;&nbsp;
            <button type="button" class="btn btn-success btn-sm" onclick="$('#target5').toggle();">Show/Hide Content
            </button>
        </h3>
        <hr style="margin-top:0;margin-bottom:30px;">
        <div id="target5" style="display: none">
            <div class="row">
                <div class="col-sm-12">
                    <p>Conventional machine learning and classification methods reply heavily on learning features from
                        still images or videos based on a single feature representation
                        of an object, hoping this representation cover up most of the aspect of an object.</p>

                </div>
            </div>
        </div>
    </div>

    <br/><br/><br/><br/>
    <!-- FOOTER -->
    <div class="footer-outer">
        <div class="container footer">
            <div class="last-modify">
                <script language="Javascript">
                    document.write("Last modified on: " + document.lastModified + "");
                </SCRIPT>
                </p>
            </div>
        </div>
    </div>
</div>
</body>
</html>
